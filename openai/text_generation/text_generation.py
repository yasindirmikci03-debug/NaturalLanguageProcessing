# Using OpenAI's text generation models, you can build applications to:

# Draft documents
# Write computer code
# Answer questions about a knowledge base
# Analyze texts
# Give software a natural language interface
# Tutor in a range of subjects
# Translate languages
# Simulate characters for games

from openai import OpenAI
import tiktoken

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role" : "system","content" : "You are a helpful assistant"},
        {"role" : "user","content" : "Who won the world series in 2020?"},
        {"role" : "assistant","content" : "The Los Angeles Dodges won the World Series in 2020."},
        {"role" : "user","content" : "Where was it played?"}
    ]
)

### JSON Mode

# When usin Json mode, always instruct the model to produce JSON via some message in the conversation.
# If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace 
# and the request may run continually until it reaches the token limit.

response2 = client.chat.completions.create(
    model="gpt-3.5-turbo-0125",
    response_format={"type" : "json_object"},
    messages=[
        {"role" : "system","content" : "You are a helpful assistant designed to output JSON."},
        {"role" : "user","content" : "Who won the world series in 2020?"}
    ]
)


### Managing Tokens

def num_tokens_from_messages(messages,model="gpt-3.5-turbo-0613"):
    """Returns the number of tokens used by a list of messages."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    
    if model == "gpt-3.5-turbo-0613":
        num_tokens = 0
        for message in messages:
            num_tokens +=4          # every message follows <im_start>{role/name}\n{content}<im_end>\n
            for key,value in message.items():
                num_tokens += len(encoding.encode(value))
                if key == "name":       # if there is a name, the role is omitted
                    num_tokens += -1
        num_tokens += 2         # every reply is primed with <im_start>assistant
        return num_tokens
    else:
        raise NotImplementedError(f"""num_tokens_from_messages() is not presently implemented for model {model}.""")
    

# Next, create a message and pass it to the function defined above to see the token count, this should match the value returned 
# by the API usage parameter:


messages = [
  {"role": "system", "content": "You are a helpful, pattern-following assistant that translates corporate jargon into plain English."},
  {"role": "system", "name":"example_user", "content": "New synergies will help drive top-line growth."},
  {"role": "system", "name": "example_assistant", "content": "Things working well together will increase revenue."},
  {"role": "system", "name":"example_user", "content": "Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage."},
  {"role": "system", "name": "example_assistant", "content": "Let's talk later when we're less busy about how to do better."},
  {"role": "user", "content": "This late pivot means we don't have time to boil the ocean for the client deliverable."},
]

model = "gpt-3.5-turbo-0613"

print(f"{num_tokens_from_messages(messages, model)} prompt tokens counted.")

# To confirm the number generated by our function above is the same as what the API returns, create a new chat completion

response3 = client.chat.completions.create(
    model = model,
    messages = messages,
    temperature = 0
)

print(f'{response.usage.prompt_tokens} prompt tokens used.')


##### Frequency and presence penalties 

# mu[j] -> mu[j] - c[j] * alpha_frequency - float(c[j] > 0) * alpha_presence

# mu[j] is the logits of the j-th token
# c[j] is how often that token was sampled prior to the current position
# float(c[j] > 0) is 1 if c[j] > 0 and 0 otherwise
# alpha_frequency is the frequency penalty coefficient
# alpha_presence is the presence penalty coefficient



##### Completions API


response4 = client.chat.completions.create(
    model = "gpt-3.5-turbo-instruct",
    prompt = "Write a tagline for an ice cream shop."
)